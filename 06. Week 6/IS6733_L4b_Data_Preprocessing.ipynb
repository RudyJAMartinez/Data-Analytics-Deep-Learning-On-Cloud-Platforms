{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.1"
    },
    "colab": {
      "name": "IS6733-L4b-Data-Preprocessing.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDwZ-1qHqM-4"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ifcvsZGSqM-5"
      },
      "source": [
        "## Categorical Features\n",
        "\n",
        "One common type of non-numerical data is *categorical* data.\n",
        "For example, imagine you are exploring some data on housing prices, and along with numerical features like \"price\" and \"rooms\", you also have \"neighborhood\" information.\n",
        "For example, your data might look something like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "JNswejAGqM-5"
      },
      "source": [
        "data = [\n",
        "    {'price': 850000, 'rooms': 4, 'neighborhood': 'Queen Anne'},\n",
        "    {'price': 700000, 'rooms': 3, 'neighborhood': 'Fremont'},\n",
        "    {'price': 650000, 'rooms': 3, 'neighborhood': 'Wallingford'},\n",
        "    {'price': 600000, 'rooms': 2, 'neighborhood': 'Fremont'}\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "U3mvT2UvqM-8"
      },
      "source": [
        "You might be tempted to encode this data with a straightforward numerical mapping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "z1Vn-2sDqM-8"
      },
      "source": [
        "{'Queen Anne': 1, 'Fremont': 2, 'Wallingford': 3};"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "1S8_8wr8qM--"
      },
      "source": [
        "It turns out that this is not generally a useful approach in Scikit-Learn: the package's models make the fundamental assumption that numerical features reflect algebraic quantities.\n",
        "Thus such a mapping would imply, for example, that *Queen Anne < Fremont < Wallingford*, or even that *Wallingford - Queen Anne = Fremont*, which (niche demographic jokes aside) does not make much sense.\n",
        "\n",
        "In this case, one proven technique is to use *one-hot encoding*, which effectively creates extra columns indicating the presence or absence of a category with a value of 1 or 0, respectively.\n",
        "When your data comes as a list of dictionaries, Scikit-Learn's ``DictVectorizer`` will do this for you:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Kjd-O6SsqM--",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f800005d-299d-4ed5-9d22-801802877c24"
      },
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "vec = DictVectorizer(sparse=False, dtype=int)\n",
        "vec.fit_transform(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[     0,      1,      0, 850000,      4],\n",
              "       [     1,      0,      0, 700000,      3],\n",
              "       [     0,      0,      1, 650000,      3],\n",
              "       [     1,      0,      0, 600000,      2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "IRH2EIWkqM_B"
      },
      "source": [
        "Notice that the 'neighborhood' column has been expanded into three separate columns, representing the three neighborhood labels, and that each row has a 1 in the column associated with its neighborhood.\n",
        "With these categorical features thus encoded, you can proceed as normal with fitting a Scikit-Learn model.\n",
        "\n",
        "To see the meaning of each column, you can inspect the feature names:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "2tFAK2gJqM_C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3fd33379-3c28-45bc-c311-475c93add6a7"
      },
      "source": [
        "vec.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['neighborhood=Fremont',\n",
              " 'neighborhood=Queen Anne',\n",
              " 'neighborhood=Wallingford',\n",
              " 'price',\n",
              " 'rooms']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "didu35uJqM_E"
      },
      "source": [
        "There is one clear disadvantage of this approach: if your category has many possible values, this can *greatly* increase the size of your dataset.\n",
        "However, because the encoded data contains mostly zeros, a sparse output can be a very efficient solution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ggNWHMBPqM_E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "58d4d1c0-4222-43d8-e6b3-ede4c9acb2b7"
      },
      "source": [
        "vec = DictVectorizer(sparse=True, dtype=int)\n",
        "vec.fit_transform(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<4x5 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 12 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "VLdGcYsRqM_H"
      },
      "source": [
        "Many (though not yet all) of the Scikit-Learn estimators accept such sparse inputs when fitting and evaluating models. ``sklearn.preprocessing.OneHotEncoder`` and ``sklearn.feature_extraction.FeatureHasher`` are two additional tools that Scikit-Learn includes to support this type of encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "VFvd6TGXqM_H"
      },
      "source": [
        "## Text Features\n",
        "\n",
        "Another common need in feature engineering is to convert text to a set of representative numerical values.\n",
        "For example, most automatic mining of social media data relies on some form of encoding the text as numbers.\n",
        "One of the simplest methods of encoding data is by *word counts*: you take each snippet of text, count the occurrences of each word within it, and put the results in a table.\n",
        "\n",
        "For example, consider the following set of three phrases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "vGkB6Sq6qM_I"
      },
      "source": [
        "sample = ['problem of evil',\n",
        "          'evil queen',\n",
        "          'horizon problem']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "9S5IGonwqM_K"
      },
      "source": [
        "For a vectorization of this data based on word count, we could construct a column representing the word \"problem,\" the word \"evil,\" the word \"horizon,\" and so on.\n",
        "While doing this by hand would be possible, the tedium can be avoided by using Scikit-Learn's ``CountVectorizer``:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "il7dFRdsqM_K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f54473cd-5408-406a-e380-d31ad09abc07"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vec = CountVectorizer()\n",
        "X = vec.fit_transform(sample)\n",
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3x5 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 7 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "2a4zc0dJqM_M"
      },
      "source": [
        "The result is a sparse matrix recording the number of times each word appears; it is easier to inspect if we convert this to a ``DataFrame`` with labeled columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "SVuOD9E0qM_M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "7f4b09b3-86b9-4091-b19e-6555ee2a9cc1"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>evil</th>\n",
              "      <th>horizon</th>\n",
              "      <th>of</th>\n",
              "      <th>problem</th>\n",
              "      <th>queen</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   evil  horizon  of  problem  queen\n",
              "0     1        0   1        1      0\n",
              "1     1        0   0        0      1\n",
              "2     0        1   0        1      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ZDCJLEgXqM_c"
      },
      "source": [
        "## Imputation of Missing Data\n",
        "\n",
        "Another common need in feature engineering is handling of missing data.\n",
        "We discussed the handling of missing data in ``DataFrame``s in [Handling Missing Data](03.04-Missing-Values.ipynb), and saw that often the ``NaN`` value is used to mark missing values.\n",
        "For example, we might have a dataset that looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "rk2HywlPqM_c"
      },
      "source": [
        "from numpy import nan\n",
        "X = np.array([[ nan, 0,   3  ],\n",
        "              [ 3,   7,   9  ],\n",
        "              [ 3,   5,   2  ],\n",
        "              [ 4,   nan, 6  ],\n",
        "              [ 8,   8,   1  ]])\n",
        "y = np.array([14, 16, -1,  8, -5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "p0yrxXUdqM_e"
      },
      "source": [
        "When applying a typical machine learning model to such data, we will need to first replace such missing data with some appropriate fill value.\n",
        "This is known as *imputation* of missing values, and strategies range from simple (e.g., replacing missing values with the mean of the column) to sophisticated (e.g., using matrix completion or a robust model to handle such data).\n",
        "\n",
        "The sophisticated approaches tend to be very application-specific, and we won't dive into them here.\n",
        "For a baseline imputation approach, using the mean, median, or most frequent value, Scikit-Learn provides the ``Imputer`` class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "8SKATm9JqM_e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ac073494-64a6-433e-d926-d21d799b1059"
      },
      "source": [
        "from sklearn.impute import SimpleImputer \n",
        "imp = SimpleImputer(strategy='mean')\n",
        "X2 = imp.fit_transform(X)\n",
        "X2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.5, 0. , 3. ],\n",
              "       [3. , 7. , 9. ],\n",
              "       [3. , 5. , 2. ],\n",
              "       [4. , 5. , 6. ],\n",
              "       [8. , 8. , 1. ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "tjLhEtpMqM_g"
      },
      "source": [
        "We see that in the resulting data, the two missing values have been replaced with the mean of the remaining values in the column. This imputed data can then be fed directly into, for example, a ``LinearRegression`` estimator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "5xna51kpqM_g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c54bea3c-a4c4-4ae1-bfc1-079796ea41eb"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression().fit(X2, y)\n",
        "model.predict(X2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([13.14869292, 14.3784627 , -1.15539732, 10.96606197, -5.33782027])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    }
  ]
}